{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_manager.data_loader_okvqa import DataLoaderOKVQA\n",
    "from utils.config_system import get_config_from_json, process_config\n",
    "import argparse\n",
    "from easydict import EasyDict\n",
    "#from data_loader_manager.data_loader_wrapper import DataLoaderWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = '/Users/tonyyan/Desktop/Tony_VQA/Tony-VQA/configs/okvqa/DPR1.jsonnet'\n",
    "config, config_dict = get_config_from_json(jsonfile)\n",
    "args = argparse.Namespace(config=jsonfile, reset='your_value', mode='your_value', \n",
    "                           experiment_name='your_value', modules=[], tags=[], \n",
    "                           test_batch_size=-1, test_evaluation_name='', opts=[])\n",
    "\n",
    "# Now pass this mock namespace to process_config\n",
    "config = process_config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoaderOKVQA(config)\n",
    "dataloader.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6269)\n",
      "4.6268815994262695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Randomly generating output and label tensors\n",
    "output = torch.randn(8, 32)  # Random logits for a batch of 8 items and 16 classes\n",
    "label = torch.randint(0, 16, (8,))  # Random true classes for each item in the batch\n",
    "#print(output)\n",
    "#print(label)\n",
    "# Cross-Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, label)\n",
    "print(loss)\n",
    "\n",
    "# Printing the loss\n",
    "print(loss.item())  # .item() is used to get the scalar value from the tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('img.pkl', 'rb') as f:\n",
    "    img = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.5885,  29.6192,   1.1034,  -8.0993, -43.5419,   5.2552, -51.0941,\n",
      "        -15.4133])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embedding_dim = 768\n",
    "batch_size = 8\n",
    "# Example embeddings for a question-image pair and a document\n",
    "# Let's assume these embeddings are obtained from their respective encoders\n",
    "Q = torch.randn(24, embedding_dim)  # Embeddings for question-image pair (size l_Q x embedding_dim)\n",
    "D = torch.randn(16, embedding_dim)  # Embeddings for document (size l_D x embedding_dim)\n",
    "intervalq = 24//batch_size\n",
    "intervald = 16//batch_size\n",
    "# Calculate the relevance score R(q, d)\n",
    "# Using broadcasting to compute the dot product between each pair of embeddings\n",
    "batch_score = torch.zeros(batch_size)\n",
    "for i in range(batch_size):\n",
    "    dot_products = torch.matmul(Q[i*intervalq:(i+1)*intervalq-1], D[i*intervald:(i+1)*intervald-1].T)  # Resulting shape will be l_Q x l_D\n",
    "    max_dot_products = torch.max(dot_products, dim=1).values\n",
    "    relevance_score = torch.sum(max_dot_products)\n",
    "    batch_score[i] = relevance_score\n",
    "print(batch_score)\n",
    "# Find the max of dot products along the document dimension\n",
    "\n",
    "\n",
    "# Sum over the question-image pair embeddings to get the final relevance score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4435, -0.8074, -1.0993, -1.3970],\n",
      "        [ 1.3275,  1.5936,  0.7492,  1.3592],\n",
      "        [ 0.3169, -0.8718, -1.2062,  2.0249],\n",
      "        [-0.3374, -0.0322, -0.5785, -1.0548]])\n",
      "tensor([[ 0.4435, -0.8074, -1.0993, -1.3970,  1.3275,  1.5936,  0.7492,  1.3592],\n",
      "        [ 0.3169, -0.8718, -1.2062,  2.0249, -0.3374, -0.0322, -0.5785, -1.0548]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "b = a.reshape(2, 8)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAVQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
