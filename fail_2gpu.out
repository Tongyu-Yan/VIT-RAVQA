Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
Changed directory to /rds/user/ty308/hpc-work/myvqa/Tony-VQA.

Current directory: /rds/user/ty308/hpc-work/myvqa/Tony-VQA

Nodes allocated:
================
gpu-q-21

numtasks=2, numnodes=1, mpi_tasks_per_node=2 (OMP_NUM_THREADS=1)

Executing command:
==================
python src/main.py configs/okvqa/RAVQA.jsonnet --mode train      --experiment_name attention_RAG     --accelerator auto --devices 2      --modules force_existence      --opts train.epochs=8              train.batch_size=1              valid.step_size=1              valid.batch_size=4              train.additional.gradient_accumulation_steps=32              train.lr=0.00006              train.retriever_lr=0.00001              train.scheduler=linear              model_config.loss_ratio.additional_loss=1              model_config.RAVQA_loss_type=Approach6              data_loader.additional.num_knowledge_passages=5

/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.
  new_rank_zero_deprecation(
/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.
  return new_rank_zero_deprecation(*args, **kwargs)
ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.
ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.
/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
[38;20m[INFO] - __main__ : Initialization done with the config: {'DATA_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data', 'EXPERIMENT_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs', 'WANDB': {'entity': 'tony-vision', 'project': 'VQA_publication', 'tags': ['OKVQA', 'RAG', 'force_existence'], 'name': 'attention_RAG'}, 'cache': {'default_folder': '/home/ty308/rds/hpc-work/data/ok-vqa/cache', 'regenerate': {'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/val2014', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_val2014_annotations.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset', 'index_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'attention_RAG', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 't5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 't5-large', 'ModelClass': 'RagModel', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}]}, 'loss_ratio': {'additional_loss': 1, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 32, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 1, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 4, 'break_interval': 3000, 'step_size': 1}, 'vit_model_config': {'VisionModelClass': 'CLIPVisionModel', 'VisionModelConfigClass': 'CLIPVisionConfig', 'VisionModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map'}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train', 'experiment_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG', 'saved_model_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model', 'imgs_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/imgs', 'tensorboard_path': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs/attention_RAG', 'args': {'config': 'configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'attention_RAG', 'tags': [], 'modules': ['force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '2', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'opts': ['train.epochs=8', 'train.batch_size=1', 'valid.step_size=1', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5']}}[0m
Global seed set to 2021
[38;20m[INFO] - __main__ : All seeds have been set to 2021[0m
[38;20m[INFO] - __main__ : init wandb logger with the following settings: {'entity': 'tony-vision', 'project': 'VQA_publication', 'tags': ['OKVQA', 'RAG', 'force_existence'], 'name': 'attention_RAG'}[0m
wandb: Currently logged in as: 793062521a (tony-vision). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /rds/user/ty308/hpc-work/myvqa/Tony-VQA/wandb/run-20240215_204607-0a5cgk0p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run attention_RAG
wandb: ⭐️ View project at https://wandb.ai/tony-vision/VQA_publication
wandb: 🚀 View run at https://wandb.ai/tony-vision/VQA_publication/runs/0a5cgk0p
Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, config='configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='2', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='attention_RAG', fast_dev_run=False, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=1', 'valid.step_size=1', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5'], overfit_batches=0.0, plugins=None, precision=32, profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, strategy=None, sync_batchnorm=False, tags=[], test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None)
input value {} is not a number, parse to string.
input value {} is not a number, parse to string.
{'DATA_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data', 'EXPERIMENT_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Experiments', 'TENSORBOARD_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs', 'WANDB': {'CACHE_DIR': '/home/ty308/rds/hpc-work/data/wandb_cache', 'entity': 'tony-vision', 'project': 'VQA_publication', 'tags': ['OKVQA']}, 'cache': {'default_folder': '/home/ty308/rds/hpc-work/data/ok-vqa/cache', 'regenerate': {'ocr_feature_preprocessed': 0, 'test_data_preprocessed': 0, 'train_data_preprocessed': 0, 'vinvl_feature_preprocessed': 0}}, 'cuda': 0, 'data_loader': {'additional': {'max_decoder_source_length': 512, 'max_source_length': 512, 'max_target_length': 10, 'num_knowledge_passages': 5}, 'dataset_modules': {'module_dict': {'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True, 'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}, 'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_test.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_train.json', 'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}}, 'option': 'default', 'type': 'LoadGoogleSearchAnnotations'}, 'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}, 'LoadOKVQAData': {'config': {'image_data_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/val2014', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_val2014_annotations.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}, 'LoadOscarCaptionFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}, 'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json', 'train': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}}, 'option': 'none', 'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'}, 'LoadVinVLFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}}, 'module_list': ['LoadVinVLFeatures', 'LoadGoogleOCRFeatures', 'LoadOscarCaptionFeatures', 'LoadOKVQAData', 'LoadGoogleSearchPassageData']}, 'dataset_type': 'OKVQADataset', 'dummy_dataloader': 0, 'index_files': {'index_passages_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset', 'index_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset_hnsw_index.faiss'}, 'type': 'DataLoaderOKVQAWithKnowledge'}, 'experiment_name': 'attention_RAG', 'gpu_device': 0, 'ignore_pretrained_weights': [], 'metrics': [{'name': 'compute_exact_match'}, {'name': 'compute_retrieval_metrics'}, {'name': 'compute_okvqa_scores'}], 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>'], 'bos_token': '<PAD>', 'pad_token': '<PAD>'}, 'DecoderTokenizerClass': 'T5Tokenizer', 'DecoderTokenizerModelVersion': 't5-large', 'GeneratorConfigClass': 'T5Config', 'GeneratorModelClass': 'T5ForConditionalGeneration', 'GeneratorModelVersion': 't5-large', 'ModelClass': 'RagModel', 'QueryEncoderConfigClass': 'DPRConfig', 'QueryEncoderModelClass': 'DPRQuestionEncoder', 'QueryEncoderModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/query_encoder', 'RAVQA_loss_type': 'Approach6', 'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>', '<SOV>', '<EOV>', '<BOQ>', '<EOQ>', '<BOC>', '<EOC>', '<BOK>', '<EOK>']}, 'TokenizerClass': 'DPRQuestionEncoderTokenizer', 'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base', 'base_model': 'RAG', 'decoder_input_modules': {'module_list': [], 'postprocess_module_list': []}, 'input_modules': {'module_list': [{'option': 'default', 'separation_tokens': {'end': '<EOQ>', 'start': '<BOQ>'}, 'type': 'QuestionInput'}, {'option': 'caption', 'separation_tokens': {'end': '<EOC>', 'start': '<BOC>'}, 'type': 'TextBasedVisionInput'}, {'attribute_max': 3, 'attribute_thres': 0.05, 'object_max': 40, 'ocr': 1, 'option': 'object', 'separation_tokens': {'end': '<EOV>', 'sep': '<SOV>', 'start': '<BOV>'}, 'type': 'TextBasedVisionInput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessInputTokenization'}]}, 'loss_ratio': {'additional_loss': 1, 'nll_loss': 1, 'rag_loss': 0, 'retrieval_pseudo_loss': 0}, 'modules': ['force_existence'], 'output_modules': {'module_list': [{'option': 'default', 'type': 'GenerationOutput'}], 'postprocess_module_list': [{'option': 'default', 'type': 'PostProcessOutputTokenization'}]}, 'pretrained': 1, 'rag_modules': {'module_list': []}}, 'platform_type': 'pytorch', 'seed': 2021, 'test': {'additional': {'multiprocessing': 4}, 'batch_size': 32, 'evaluation_name': 'test_evaluation', 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'num_evaluation': 0}, 'train': {'adam_epsilon': 1e-08, 'additional': {'gradient_accumulation_steps': 32, 'gradient_clipping': 0, 'plugins': [], 'save_top_k': 1, 'save_top_k_metric': 'test/accuracy_overall', 'save_top_k_mode': 'max', 'warmup_steps': 0}, 'batch_size': 1, 'epochs': 8, 'load_best_model': 0, 'load_epoch': -1, 'load_model_path': '', 'lr': 6e-05, 'retriever_lr': 1e-05, 'save_interval': 1, 'scheduler': 'linear', 'type': 'RagExecutor'}, 'valid': {'additional': {}, 'batch_size': 4, 'break_interval': 3000, 'step_size': 1}, 'vit_model_config': {'VisionModelClass': 'CLIPVisionModel', 'VisionModelConfigClass': 'CLIPVisionConfig', 'VisionModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map'}, 'reset': False, 'mode': 'train', 'log_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train', 'experiment_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG', 'saved_model_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model', 'imgs_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/imgs', 'tensorboard_path': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs/attention_RAG', 'args': {'config': 'configs/okvqa/RAVQA.jsonnet', 'DATA_FOLDER': '', 'EXPERIMENT_FOLDER': '', 'mode': 'train', 'reset': False, 'experiment_name': 'attention_RAG', 'tags': [], 'modules': ['force_existence'], 'log_prediction_tables': False, 'test_batch_size': -1, 'test_evaluation_name': '', 'logger': True, 'enable_checkpointing': True, 'default_root_dir': None, 'gradient_clip_val': None, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '2', 'gpus': None, 'auto_select_gpus': False, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 1, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 50, 'accelerator': 'auto', 'strategy': None, 'sync_batchnorm': False, 'precision': 32, 'enable_model_summary': True, 'weights_save_path': None, 'num_sanity_val_steps': 2, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'deterministic': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': True, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': 'native', 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'opts': ['train.epochs=8', 'train.batch_size=1', 'valid.step_size=1', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5']}}
['/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train', '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model', '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/imgs', '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs/attention_RAG']
{'DATA_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data',
 'EXPERIMENT_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Experiments',
 'TENSORBOARD_FOLDER': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs',
 'WANDB': {'entity': 'tony-vision',
           'name': 'attention_RAG',
           'project': 'VQA_publication',
           'tags': ['OKVQA', 'RAG', 'force_existence']},
 'args': {'DATA_FOLDER': '',
          'EXPERIMENT_FOLDER': '',
          'accelerator': 'auto',
          'accumulate_grad_batches': None,
          'amp_backend': 'native',
          'amp_level': None,
          'auto_lr_find': False,
          'auto_scale_batch_size': False,
          'auto_select_gpus': False,
          'benchmark': None,
          'check_val_every_n_epoch': 1,
          'config': 'configs/okvqa/RAVQA.jsonnet',
          'default_root_dir': None,
          'detect_anomaly': False,
          'deterministic': None,
          'devices': '2',
          'enable_checkpointing': True,
          'enable_model_summary': True,
          'enable_progress_bar': True,
          'experiment_name': 'attention_RAG',
          'fast_dev_run': False,
          'gpus': None,
          'gradient_clip_algorithm': None,
          'gradient_clip_val': None,
          'ipus': None,
          'limit_predict_batches': None,
          'limit_test_batches': None,
          'limit_train_batches': None,
          'limit_val_batches': None,
          'log_every_n_steps': 50,
          'log_prediction_tables': False,
          'logger': True,
          'max_epochs': None,
          'max_steps': -1,
          'max_time': None,
          'min_epochs': None,
          'min_steps': None,
          'mode': 'train',
          'modules': ['force_existence'],
          'move_metrics_to_cpu': False,
          'multiple_trainloader_mode': 'max_size_cycle',
          'num_nodes': 1,
          'num_processes': None,
          'num_sanity_val_steps': 2,
          'opts': ['train.epochs=8',
                   'train.batch_size=1',
                   'valid.step_size=1',
                   'valid.batch_size=4',
                   'train.additional.gradient_accumulation_steps=32',
                   'train.lr=0.00006',
                   'train.retriever_lr=0.00001',
                   'train.scheduler=linear',
                   'model_config.loss_ratio.additional_loss=1',
                   'model_config.RAVQA_loss_type=Approach6',
                   'data_loader.additional.num_knowledge_passages=5'],
          'overfit_batches': 0.0,
          'plugins': None,
          'precision': 32,
          'profiler': None,
          'reload_dataloaders_every_n_epochs': 0,
          'replace_sampler_ddp': True,
          'reset': False,
          'resume_from_checkpoint': None,
          'strategy': None,
          'sync_batchnorm': False,
          'tags': [],
          'test_batch_size': -1,
          'test_evaluation_name': '',
          'tpu_cores': None,
          'track_grad_norm': -1,
          'val_check_interval': None,
          'weights_save_path': None},
 'cache': {'default_folder': '/home/ty308/rds/hpc-work/data/ok-vqa/cache',
           'regenerate': {'ocr_feature_preprocessed': 0,
                          'test_data_preprocessed': 0,
                          'train_data_preprocessed': 0,
                          'vinvl_feature_preprocessed': 0}},
 'cuda': 0,
 'data_loader': {'additional': {'max_decoder_source_length': 512,
                                'max_source_length': 512,
                                'max_target_length': 10,
                                'num_knowledge_passages': 5},
                 'dataset_modules': {'module_dict': {'LoadGoogleOCRFeatures': {'config': {'combine_with_vinvl': True,
                                                                                          'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/valid',
                                                                                          'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/train'},
                                                                               'option': 'default',
                                                                               'type': 'LoadGoogleOCRFeatures'},
                                                     'LoadGoogleSearchAnnotations': {'config': {'annotations_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_test.json',
                                                                                                                     'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_train.json',
                                                                                                                     'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/retriever_testdev.json'}},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchAnnotations'},
                                                     'LoadGoogleSearchPassageData': {'config': {'passage_data_path': {'full': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv',
                                                                                                                      'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'},
                                                                                                'use_full_split': True},
                                                                                     'option': 'default',
                                                                                     'type': 'LoadGoogleSearchPassageData'},
                                                     'LoadOKVQAData': {'config': {'image_data_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/val2014',
                                                                                                      'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/train2014'},
                                                                                  'vqa_data_path': {'annotation_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_val2014_annotations.json',
                                                                                                                         'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_train2014_annotations.json'},
                                                                                                    'question_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_val2014_questions.json',
                                                                                                                       'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}},
                                                                       'option': 'default',
                                                                       'type': 'LoadOKVQAData'},
                                                     'LoadOscarCaptionFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/test_predictions.json',
                                                                                             'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/train_predictions.json',
                                                                                             'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'},
                                                                                  'option': 'default',
                                                                                  'type': 'LoadOscarCaptionFeatures'},
                                                     'LoadPretrainedDPROutputForGoogleSearchPassage': {'config': {'pretrained_dpr_outputs': {'test': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/test_predictions.json',
                                                                                                                                             'train': '/home/ty308/rds/hpc-work/myvqa/Experiments/Knowledge_Retriever_DPR_dim_768_inbatch_negative_caption_FullCorpus_NewRun/test/test_evaluation/train_predictions.json'}},
                                                                                                       'option': 'none',
                                                                                                       'type': 'LoadPretrainedDPROutputForGoogleSearchPassage'},
                                                     'LoadVinVLFeatures': {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv',
                                                                                      'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'},
                                                                           'option': 'default',
                                                                           'type': 'LoadVinVLFeatures'}},
                                     'module_list': ['LoadVinVLFeatures',
                                                     'LoadGoogleOCRFeatures',
                                                     'LoadOscarCaptionFeatures',
                                                     'LoadOKVQAData',
                                                     'LoadGoogleSearchPassageData']},
                 'dataset_type': 'OKVQADataset',
                 'dummy_dataloader': 0,
                 'index_files': {'index_passages_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset',
                                 'index_path': '/home/ty308/rds/hpc-work/data/ok-vqa/pre-extracted_features/faiss_attention/ok-vqa-passages-full-new-framework/my_knowledge_dataset_hnsw_index.faiss'},
                 'type': 'DataLoaderOKVQAWithKnowledge'},
 'experiment_name': 'attention_RAG',
 'experiment_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG',
 'gpu_device': 0,
 'ignore_pretrained_weights': [],
 'imgs_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/imgs',
 'log_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train',
 'metrics': [{'name': 'compute_exact_match'},
             {'name': 'compute_retrieval_metrics'},
             {'name': 'compute_okvqa_scores'}],
 'mode': 'train',
 'model_config': {'DECODER_SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>',
                                                                           '<SOV>',
                                                                           '<EOV>',
                                                                           '<BOQ>',
                                                                           '<EOQ>',
                                                                           '<BOC>',
                                                                           '<EOC>',
                                                                           '<BOK>',
                                                                           '<EOK>'],
                                             'bos_token': '<PAD>',
                                             'pad_token': '<PAD>'},
                  'DecoderTokenizerClass': 'T5Tokenizer',
                  'DecoderTokenizerModelVersion': 't5-large',
                  'GeneratorConfigClass': 'T5Config',
                  'GeneratorModelClass': 'T5ForConditionalGeneration',
                  'GeneratorModelVersion': 't5-large',
                  'ModelClass': 'RagModel',
                  'QueryEncoderConfigClass': 'DPRConfig',
                  'QueryEncoderModelClass': 'DPRQuestionEncoder',
                  'QueryEncoderModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/query_encoder',
                  'RAVQA_loss_type': 'Approach6',
                  'SPECIAL_TOKENS': {'additional_special_tokens': ['<BOV>',
                                                                   '<SOV>',
                                                                   '<EOV>',
                                                                   '<BOQ>',
                                                                   '<EOQ>',
                                                                   '<BOC>',
                                                                   '<EOC>',
                                                                   '<BOK>',
                                                                   '<EOK>']},
                  'TokenizerClass': 'DPRQuestionEncoderTokenizer',
                  'TokenizerModelVersion': 'facebook/dpr-question_encoder-single-nq-base',
                  'base_model': 'RAG',
                  'decoder_input_modules': {'module_list': [],
                                            'postprocess_module_list': []},
                  'input_modules': {'module_list': [{'option': 'default',
                                                     'separation_tokens': {'end': '<EOQ>',
                                                                           'start': '<BOQ>'},
                                                     'type': 'QuestionInput'},
                                                    {'option': 'caption',
                                                     'separation_tokens': {'end': '<EOC>',
                                                                           'start': '<BOC>'},
                                                     'type': 'TextBasedVisionInput'},
                                                    {'attribute_max': 3,
                                                     'attribute_thres': 0.05,
                                                     'object_max': 40,
                                                     'ocr': 1,
                                                     'option': 'object',
                                                     'separation_tokens': {'end': '<EOV>',
                                                                           'sep': '<SOV>',
                                                                           'start': '<BOV>'},
                                                     'type': 'TextBasedVisionInput'}],
                                    'postprocess_module_list': [{'option': 'default',
                                                                 'type': 'PostProcessInputTokenization'}]},
                  'loss_ratio': {'additional_loss': 1,
                                 'nll_loss': 1,
                                 'rag_loss': 0,
                                 'retrieval_pseudo_loss': 0},
                  'modules': ['force_existence'],
                  'output_modules': {'module_list': [{'option': 'default',
                                                      'type': 'GenerationOutput'}],
                                     'postprocess_module_list': [{'option': 'default',
                                                                  'type': 'PostProcessOutputTokenization'}]},
                  'pretrained': 1,
                  'rag_modules': {'module_list': []}},
 'platform_type': 'pytorch',
 'reset': False,
 'saved_model_path': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model',
 'seed': 2021,
 'tensorboard_path': '/rds/user/ty308/hpc-work/myvqa/Data_TB/tb_logs/attention_RAG',
 'test': {'additional': {'multiprocessing': 4},
          'batch_size': 32,
          'evaluation_name': 'test_evaluation',
          'load_best_model': 0,
          'load_epoch': -1,
          'load_model_path': '',
          'num_evaluation': 0},
 'train': {'adam_epsilon': 1e-08,
           'additional': {'gradient_accumulation_steps': 32,
                          'gradient_clipping': 0,
                          'plugins': [],
                          'save_top_k': 1,
                          'save_top_k_metric': 'test/accuracy_overall',
                          'save_top_k_mode': 'max',
                          'warmup_steps': 0},
           'batch_size': 1,
           'epochs': 8,
           'load_best_model': 0,
           'load_epoch': -1,
           'load_model_path': '',
           'lr': 6e-05,
           'retriever_lr': 1e-05,
           'save_interval': 1,
           'scheduler': 'linear',
           'type': 'RagExecutor'},
 'valid': {'additional': {},
           'batch_size': 4,
           'break_interval': 3000,
           'step_size': 1},
 'vit_model_config': {'VisionModelClass': 'CLIPVisionModel',
                      'VisionModelConfigClass': 'CLIPVisionConfig',
                      'VisionModelVersion': '/home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map'}}
/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.
  new_rank_zero_deprecation(
/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.LightningLoggerBase` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.Logger` instead.
  return new_rank_zero_deprecation(*args, **kwargs)
Multiprocessing is handled by SLURM.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;20m[INFO] - __main__ : arguments passed to trainer: Namespace(DATA_FOLDER='', EXPERIMENT_FOLDER='', accelerator='auto', accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, config='configs/okvqa/RAVQA.jsonnet', default_root_dir=None, detect_anomaly=False, deterministic=None, devices='2', enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, experiment_name='attention_RAG', fast_dev_run=False, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_prediction_tables=False, logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='train', modules=['force_existence'], move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, opts=['train.epochs=8', 'train.batch_size=1', 'valid.step_size=1', 'valid.batch_size=4', 'train.additional.gradient_accumulation_steps=32', 'train.lr=0.00006', 'train.retriever_lr=0.00001', 'train.scheduler=linear', 'model_config.loss_ratio.additional_loss=1', 'model_config.RAVQA_loss_type=Approach6', 'data_loader.additional.num_knowledge_passages=5'], overfit_batches=0.0, plugins=None, precision=32, profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, reset=False, resume_from_checkpoint=None, strategy=None, sync_batchnorm=False, tags=[], test_batch_size=-1, test_evaluation_name='', tpu_cores=None, track_grad_norm=-1, val_check_interval=None, weights_save_path=None)[0m
[38;20m[INFO] - __main__ : additional arguments passed to trainer: {'accumulate_grad_batches': 32, 'default_root_dir': '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model', 'max_epochs': 8, 'logger': [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x1518738b2df0>, <pytorch_lightning.loggers.wandb.WandbLogger object at 0x1518738c8f70>, <utils.metrics_log_callback.MetricsHistoryLogger object at 0x1518737db6a0>], 'callbacks': [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x1518738c8d00>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x15187372fdc0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x15187372f790>, <pytorch_lightning.callbacks.gradient_accumulation_scheduler.GradientAccumulationScheduler object at 0x15187372ffd0>], 'plugins': [], 'log_every_n_steps': 10, 'accelerator': 'gpu', 'strategy': 'ddp', 'devices': 2}[0m
[33;20m[WARNING] - __main__ : No checkpoint exists from '/rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG/train/saved_model/last.ckpt'. Skipping...[0m
[38;20m[INFO] - __main__ : **First time to train**[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_testset_full/inference/vinvl_vg_x152c4/predictions.tsv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/vinvl_output/vinvl_okvqa_trainset_full/inference/vinvl_vg_x152c4/predictions.tsv'}, 'option': 'default', 'type': 'LoadVinVLFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from /home/ty308/rds/hpc-work/data/ok-vqa/cache/vinvl_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] VinVL features 14031[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'combine_with_vinvl': True, 'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/valid', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/OCR/train'}, 'option': 'default', 'type': 'LoadGoogleOCRFeatures'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from /home/ty308/rds/hpc-work/data/ok-vqa/cache/ocr_feature_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data Statistics] OCR features 14031, 5462 has annotations.[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : OCR feature detected in VinVL feature dict...skipping..[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/test_predictions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/train_predictions.json', 'valid': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/captions/valid_predictions.json'}, 'option': 'default', 'type': 'LoadOscarCaptionFeatures'}[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'image_data_path': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/val2014', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/train2014'}, 'vqa_data_path': {'annotation_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_val2014_annotations.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/mscoco_train2014_annotations.json'}, 'question_files': {'test': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_val2014_questions.json', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/OpenEnded_mscoco_train2014_questions.json'}}}, 'option': 'default', 'type': 'LoadOKVQAData'}[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from /home/ty308/rds/hpc-work/data/ok-vqa/cache/train_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: train  entries: 9009[0m
[38;20m[INFO] - utils.cache_system : reading preprocessed data from /home/ty308/rds/hpc-work/data/ok-vqa/cache/test_data_preprocessed.pkl[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa : [Data statistics] split: test  entries: 5046[0m
[38;20m[INFO] - data_loader_manager.data_loader_wrapper : Loading dataset module: {'config': {'passage_data_path': {'full': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_full_corpus.csv', 'train': '/home/ty308/rds/rds-cvnlp-hirYTW1FQIw/shared_space/vqa_data/KBVQA_data/ok-vqa/pre-extracted_features/passages/okvqa_train_corpus.csv'}, 'use_full_split': True}, 'option': 'default', 'type': 'LoadGoogleSearchPassageData'}[0m
data columns: dict_keys(['vinvl_features'])
data columns: dict_keys(['vinvl_features', 'ocr_features'])
[Data Statistics] Caption features 123287
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features'])
loading VQA annotations and questions into memory...
time elpased:  0:00:00.204928
creating index...
index created!
loading VQA annotations and questions into memory...
time elpased:  0:00:00.128478
creating index...
index created!
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
creating index...
index created!
year: 2019
version: 1.0
description: This is v1.0 of the OK-VQA dataset.
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data'])
  0%|          | 0/168380 [00:00<?, ?it/s] 10%|▉         | 16653/168380 [00:00<00:00, 166510.71it/s] 20%|█▉        | 33305/168380 [00:00<00:00, 164114.39it/s] 30%|██▉       | 49719/168380 [00:00<00:00, 160707.72it/s] 39%|███▉      | 66387/168380 [00:00<00:00, 163021.01it/s] 49%|████▉     | 83236/168380 [00:00<00:00, 164968.47it/s] 59%|█████▉    | 99741/168380 [00:00<00:00, 164900.12it/s] 70%|██████▉   | 117205/168380 [00:00<00:00, 168064.98it/s] 81%|████████  | 136705/168380 [00:00<00:00, 176606.64it/s] 93%|█████████▎| 156634/168380 [00:00<00:00, 183677.49it/s]100%|█████████▉| 168307/168380 [00:00<00:00, 174800.83it/s]
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.datasets.okvqa_datasets : initialising OKVQADataset...[0m
[38;20m[INFO] - data_loader_manager.data_loader_okvqa_with_knowledge : [Data Statistics]: training data loader: 9009;  test data loader: 1262[0m
[38;20m[INFO] - trainers.base_executor : Initializing RagExecutor...[0m
Some weights of the model checkpoint at /home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map were not used when initializing CLIPVisionModel: ['vit.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.3.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vit.vision_model.post_layernorm.weight', 'vit.vision_model.encoder.layers.8.layer_norm2.bias', 'vit.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.11.mlp.fc2.bias', 'vit.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vit.vision_model.post_layernorm.bias', 'vit.vision_model.encoder.layers.4.layer_norm2.weight', 'vit.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.0.mlp.fc1.weight', 'vit.vision_model.encoder.layers.1.layer_norm1.bias', 'vit.vision_model.encoder.layers.6.layer_norm2.weight', 'vit.vision_model.encoder.layers.7.mlp.fc2.weight', 'vit.vision_model.encoder.layers.5.mlp.fc1.weight', 'vit.vision_model.encoder.layers.8.layer_norm2.weight', 'vit.vision_model.encoder.layers.2.layer_norm2.weight', 'vit.vision_model.encoder.layers.6.mlp.fc2.bias', 'vit.vision_model.embeddings.patch_embedding.weight', 'vit.vision_model.encoder.layers.7.layer_norm1.bias', 'vit.vision_model.encoder.layers.2.mlp.fc1.weight', 'vit.vision_model.encoder.layers.4.mlp.fc1.bias', 'vit.vision_model.encoder.layers.5.layer_norm1.weight', 'vit.vision_model.encoder.layers.5.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.layer_norm2.weight', 'vit.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc2.weight', 'vit.vision_model.encoder.layers.10.mlp.fc1.weight', 'vit.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.9.mlp.fc2.weight', 'vit.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.0.layer_norm2.bias', 'vit.vision_model.encoder.layers.4.layer_norm1.bias', 'vit.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.2.layer_norm2.bias', 'vit.vision_model.encoder.layers.3.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.mlp.fc1.weight', 'vit.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.8.layer_norm1.weight', 'vit.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.2.mlp.fc2.weight', 'vit.vision_model.encoder.layers.6.layer_norm2.bias', 'vit.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.mlp.fc1.bias', 'vit.vision_model.encoder.layers.9.mlp.fc2.bias', 'vit.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.7.mlp.fc1.weight', 'vit.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.5.mlp.fc2.bias', 'vit.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.layer_norm1.weight', 'vit.vision_model.encoder.layers.2.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.9.mlp.fc1.weight', 'vit.vision_model.encoder.layers.10.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.10.layer_norm2.bias', 'vit.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc1.weight', 'vit.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.6.layer_norm1.weight', 'vit.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vit.vision_model.embeddings.position_ids', 'vit.vision_model.encoder.layers.9.layer_norm2.bias', 'vit.vision_model.encoder.layers.6.layer_norm1.bias', 'vit.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.2.layer_norm1.weight', 'vit.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.9.layer_norm1.weight', 'vit.vision_model.encoder.layers.8.layer_norm1.bias', 'vit.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.4.mlp.fc2.weight', 'vit.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vit.vision_model.embeddings.position_embedding.weight', 'vit.vision_model.encoder.layers.7.layer_norm2.bias', 'map.bias', 'vit.vision_model.encoder.layers.4.mlp.fc1.weight', 'vit.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.3.mlp.fc1.weight', 'vit.vision_model.embeddings.class_embedding', 'vit.vision_model.encoder.layers.1.mlp.fc2.bias', 'vit.vision_model.encoder.layers.10.layer_norm1.weight', 'vit.vision_model.encoder.layers.3.mlp.fc2.weight', 'vit.vision_model.encoder.layers.3.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.layer_norm1.bias', 'vit.vision_model.encoder.layers.2.layer_norm1.bias', 'vit.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.6.mlp.fc1.bias', 'vit.vision_model.encoder.layers.6.mlp.fc2.weight', 'vit.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vit.vision_model.pre_layrnorm.weight', 'vit.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.10.mlp.fc1.bias', 'vit.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.3.layer_norm1.weight', 'vit.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.layer_norm1.weight', 'vit.vision_model.encoder.layers.10.mlp.fc2.weight', 'vit.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.4.layer_norm2.bias', 'vit.vision_model.encoder.layers.0.mlp.fc1.bias', 'vit.vision_model.encoder.layers.5.layer_norm1.bias', 'vit.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vit.vision_model.pre_layrnorm.bias', 'vit.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc1.bias', 'vit.vision_model.encoder.layers.0.mlp.fc2.bias', 'vit.vision_model.encoder.layers.5.mlp.fc2.weight', 'vit.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.7.mlp.fc2.bias', 'vit.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.7.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm1.bias', 'vit.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.9.layer_norm2.weight', 'map.weight', 'vit.vision_model.encoder.layers.5.layer_norm2.bias', 'vit.vision_model.encoder.layers.0.mlp.fc2.weight', 'vit.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.layer_norm1.weight', 'vit.vision_model.encoder.layers.2.mlp.fc2.bias', 'vit.vision_model.encoder.layers.6.mlp.fc1.weight', 'vit.vision_model.encoder.layers.3.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm1.weight', 'vit.vision_model.encoder.layers.8.mlp.fc1.weight', 'vit.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.mlp.fc2.weight', 'vit.vision_model.encoder.layers.10.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.4.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.8.mlp.fc1.bias', 'vit.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.mlp.fc1.bias', 'vit.vision_model.encoder.layers.8.mlp.fc2.bias', 'vit.vision_model.encoder.layers.5.layer_norm2.weight', 'vit.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.3.layer_norm2.bias', 'vit.vision_model.encoder.layers.9.mlp.fc1.bias', 'vit.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.1.layer_norm2.bias', 'vit.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.10.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.layer_norm2.bias', 'vit.vision_model.encoder.layers.9.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.1.layer_norm1.weight', 'vit.vision_model.encoder.layers.11.layer_norm2.weight', 'vit.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.8.mlp.fc2.weight', 'vit.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.7.self_attn.out_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPVisionModel were not initialized from the model checkpoint at /home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map and are newly initialized: ['vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at /home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map were not used when initializing CLIPVisionModel: ['vit.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.3.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vit.vision_model.post_layernorm.weight', 'vit.vision_model.encoder.layers.8.layer_norm2.bias', 'vit.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.11.mlp.fc2.bias', 'vit.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vit.vision_model.post_layernorm.bias', 'vit.vision_model.encoder.layers.4.layer_norm2.weight', 'vit.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.0.mlp.fc1.weight', 'vit.vision_model.encoder.layers.1.layer_norm1.bias', 'vit.vision_model.encoder.layers.6.layer_norm2.weight', 'vit.vision_model.encoder.layers.7.mlp.fc2.weight', 'vit.vision_model.encoder.layers.5.mlp.fc1.weight', 'vit.vision_model.encoder.layers.8.layer_norm2.weight', 'vit.vision_model.encoder.layers.2.layer_norm2.weight', 'vit.vision_model.encoder.layers.6.mlp.fc2.bias', 'vit.vision_model.embeddings.patch_embedding.weight', 'vit.vision_model.encoder.layers.7.layer_norm1.bias', 'vit.vision_model.encoder.layers.2.mlp.fc1.weight', 'vit.vision_model.encoder.layers.4.mlp.fc1.bias', 'vit.vision_model.encoder.layers.5.layer_norm1.weight', 'vit.vision_model.encoder.layers.5.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.layer_norm2.weight', 'vit.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc2.weight', 'vit.vision_model.encoder.layers.10.mlp.fc1.weight', 'vit.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.9.mlp.fc2.weight', 'vit.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.0.layer_norm2.bias', 'vit.vision_model.encoder.layers.4.layer_norm1.bias', 'vit.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.2.layer_norm2.bias', 'vit.vision_model.encoder.layers.3.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.mlp.fc1.weight', 'vit.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.8.layer_norm1.weight', 'vit.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.2.mlp.fc2.weight', 'vit.vision_model.encoder.layers.6.layer_norm2.bias', 'vit.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.mlp.fc1.bias', 'vit.vision_model.encoder.layers.9.mlp.fc2.bias', 'vit.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.7.mlp.fc1.weight', 'vit.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.5.mlp.fc2.bias', 'vit.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.layer_norm1.weight', 'vit.vision_model.encoder.layers.2.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.9.mlp.fc1.weight', 'vit.vision_model.encoder.layers.10.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.10.layer_norm2.bias', 'vit.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc1.weight', 'vit.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.6.layer_norm1.weight', 'vit.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vit.vision_model.embeddings.position_ids', 'vit.vision_model.encoder.layers.9.layer_norm2.bias', 'vit.vision_model.encoder.layers.6.layer_norm1.bias', 'vit.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.2.layer_norm1.weight', 'vit.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.9.layer_norm1.weight', 'vit.vision_model.encoder.layers.8.layer_norm1.bias', 'vit.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.4.mlp.fc2.weight', 'vit.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vit.vision_model.embeddings.position_embedding.weight', 'vit.vision_model.encoder.layers.7.layer_norm2.bias', 'map.bias', 'vit.vision_model.encoder.layers.4.mlp.fc1.weight', 'vit.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.3.mlp.fc1.weight', 'vit.vision_model.embeddings.class_embedding', 'vit.vision_model.encoder.layers.1.mlp.fc2.bias', 'vit.vision_model.encoder.layers.10.layer_norm1.weight', 'vit.vision_model.encoder.layers.3.mlp.fc2.weight', 'vit.vision_model.encoder.layers.3.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.layer_norm1.bias', 'vit.vision_model.encoder.layers.2.layer_norm1.bias', 'vit.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.6.mlp.fc1.bias', 'vit.vision_model.encoder.layers.6.mlp.fc2.weight', 'vit.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vit.vision_model.pre_layrnorm.weight', 'vit.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.10.mlp.fc1.bias', 'vit.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.3.layer_norm1.weight', 'vit.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.layer_norm1.weight', 'vit.vision_model.encoder.layers.10.mlp.fc2.weight', 'vit.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.4.layer_norm2.bias', 'vit.vision_model.encoder.layers.0.mlp.fc1.bias', 'vit.vision_model.encoder.layers.5.layer_norm1.bias', 'vit.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vit.vision_model.pre_layrnorm.bias', 'vit.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.mlp.fc1.bias', 'vit.vision_model.encoder.layers.0.mlp.fc2.bias', 'vit.vision_model.encoder.layers.5.mlp.fc2.weight', 'vit.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.7.mlp.fc2.bias', 'vit.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.7.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm1.bias', 'vit.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vit.vision_model.encoder.layers.9.layer_norm2.weight', 'map.weight', 'vit.vision_model.encoder.layers.5.layer_norm2.bias', 'vit.vision_model.encoder.layers.0.mlp.fc2.weight', 'vit.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.4.layer_norm1.weight', 'vit.vision_model.encoder.layers.2.mlp.fc2.bias', 'vit.vision_model.encoder.layers.6.mlp.fc1.weight', 'vit.vision_model.encoder.layers.3.mlp.fc1.bias', 'vit.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm1.weight', 'vit.vision_model.encoder.layers.8.mlp.fc1.weight', 'vit.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.0.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.mlp.fc2.weight', 'vit.vision_model.encoder.layers.10.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.4.mlp.fc2.bias', 'vit.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.8.mlp.fc1.bias', 'vit.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.11.mlp.fc1.bias', 'vit.vision_model.encoder.layers.8.mlp.fc2.bias', 'vit.vision_model.encoder.layers.5.layer_norm2.weight', 'vit.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.3.layer_norm2.bias', 'vit.vision_model.encoder.layers.9.mlp.fc1.bias', 'vit.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vit.vision_model.encoder.layers.1.layer_norm2.bias', 'vit.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vit.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vit.vision_model.encoder.layers.10.layer_norm2.weight', 'vit.vision_model.encoder.layers.11.layer_norm2.bias', 'vit.vision_model.encoder.layers.9.layer_norm1.bias', 'vit.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vit.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.1.layer_norm1.weight', 'vit.vision_model.encoder.layers.11.layer_norm2.weight', 'vit.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vit.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vit.vision_model.encoder.layers.8.mlp.fc2.weight', 'vit.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vit.vision_model.encoder.layers.7.self_attn.out_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPVisionModel were not initialized from the model checkpoint at /home/ty308/rds/hpc-work/myvqa/Experiments/V_DPR/train/saved_model/epoch5/map and are newly initialized: ['vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[38;20m[INFO] - __main__ : config file was successfully saved to /rds/user/ty308/hpc-work/myvqa/Experiments/attention_RAG for future use.[0m
Global seed set to 2021
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[38;20m[INFO] - torch.distributed.distributed_c10d : Added key: store_based_barrier_key:1 to store for rank: 0[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
[38;20m[INFO] - torch.distributed.distributed_c10d : Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
data columns: dict_keys(['vinvl_features', 'ocr_features', 'caption_features', 'okvqa_data', 'vqa_data', 'passages'])
initializing retrieval
Traceback (most recent call last):
  File "src/main.py", line 397, in <module>
    main(config)
  File "src/main.py", line 175, in main
    trainer.fit(
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1104, in _run
    self.strategy.setup_environment()
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 207, in setup_distributed
    init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 374, in init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 627, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 255, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
[31;20m[ERROR] - __main__ : Uncaught exception: <class 'RuntimeError'> --> Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)[0m
Traceback (most recent call last):
  File "src/main.py", line 397, in <module>
    main(config)
  File "src/main.py", line 175, in main
    trainer.fit(
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1104, in _run
    self.strategy.setup_environment()
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 207, in setup_distributed
    init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 374, in init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 627, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/ty308/.conda/envs/RAVQA2/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 255, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)
wandb: 🚀 View run attention_RAG at: https://wandb.ai/tony-vision/VQA_publication/runs/0a5cgk0p
wandb: ️⚡ View job at https://wandb.ai/tony-vision/VQA_publication/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDQwMTM0OQ==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240215_204607-0a5cgk0p/logs
